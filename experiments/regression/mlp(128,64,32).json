{
  "model_name": "MLP (128,64,32)",
  "base_model": "MLP",
  "lr": 0.001,
  "batch_size": 32,
  "num_epochs": 20,
  "hidden_dims": [128, 64, 32],
  "dropout": 0.5
}